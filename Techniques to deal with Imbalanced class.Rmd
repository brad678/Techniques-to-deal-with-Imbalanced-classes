---
title: "Imbalanced class - sampling techniques"
output: 
  html_document: 
    smart: no
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### load the data

```{r}
library(ROSE)
data(hacide)
str(hacide.train)

```


#### check classes distribution

```{r}
table(hacide.train$cls)
prop.table(table(hacide.train$cls))

```

#### using decision tree on imbalanced classes


```{r}

library(rpart)
treeimb <- rpart(cls ~ ., data = hacide.train)
pred.treeimb <- predict(treeimb, newdata = hacide.test)
head(pred.treeimb)

```

#### error measures


```{r}
library(caret)
library(e1071)

#using threshold of 0.5
glm.default=rep("0",length(pred.treeimb[,2]))
glm.default[pred.treeimb[,2]>0.5] <- "1"

confusionMatrix(glm.default,hacide.test$cls,positive = "1")

#Metrics to evaluate a classifier accuracy in imbalanced learning
accuracy.meas(hacide.test$cls, pred.treeimb[,2])

roc.curve(hacide.test$cls, pred.treeimb[,2], plotit = T)


```

- AUC = 0.60 is a terribly low score. Therefore, it is necessary to balance the data before applying a machine learning algorithm.


#### over sampling

```{r}

data_balanced_over <- ovun.sample(cls ~ ., data = hacide.train, method = "over",N = 1960)$data

table(data_balanced_over$cls)


```

- In the code above, method over instructs the algorithm to perform over sampling. N refers to number of observations in the resulting balanced set. In this case, originally we had 980 negative observations. So, I instructed this line of code to over sample minority class until it reaches 980 and the total data set comprises of 1960 samples.


#### under sampling

```{r}

data_balanced_under <- ovun.sample(cls ~ ., data = hacide.train, method = "under",N = 40,seed=1)$data

table(data_balanced_under$cls)


```

- In the code above, method over instructs the algorithm to perform under sampling. N refers to number of observations in the resulting balanced set. In this case, originally we had 20 positive observations. So, I instructed this line of code to under sample majority class until it reaches 20 and the total data set comprises of 40 samples.

#### Both under and over sampling

```{r}

data_balanced_both <- ovun.sample(cls ~ ., data = hacide.train, method = "both",N = 1000,seed=1)$data

table(data_balanced_both$cls)

```

- The data generated from oversampling have expected amount of repeated observations. 
- Data generated from undersampling is deprived of important information from the original data. This leads to inaccuracies in the resulting performance. 
- To encounter these issues, ROSE helps us to generate data synthetically as well. 
- The data generated using ROSE is considered to provide better estimate of original data


#### ROSE

```{r}

data.rose <- ROSE(cls ~ ., data = hacide.train, seed = 1)$data
table(data.rose$cls)


```


#### build decision tree models

```{r}

tree.rose <- rpart(cls ~ ., data = data.rose)
tree.over <- rpart(cls ~ ., data = data_balanced_over)
tree.under <- rpart(cls ~ ., data = data_balanced_under)
tree.both <- rpart(cls ~ ., data = data_balanced_both)


```


#### make predictions on unseen data

```{r}

pred.tree.rose <- predict(tree.rose, newdata = hacide.test)
pred.tree.over <- predict(tree.over, newdata = hacide.test)
pred.tree.under <- predict(tree.under, newdata = hacide.test)
pred.tree.both <- predict(tree.both, newdata = hacide.test)


```


#### evaluating the models using roc curve

```{r}
#AUC ROSE
roc.curve(hacide.test$cls, pred.tree.rose[,2])

#AUC Oversampling
roc.curve(hacide.test$cls, pred.tree.over[,2])

#AUC Undersampling
roc.curve(hacide.test$cls, pred.tree.under[,2])

#AUC Both
roc.curve(hacide.test$cls, pred.tree.both[,2])

```

- Hence, we get the highest accuracy from data obtained using ROSE algorithm. 
- We see that the data generated using synthetic methods result in high accuracy as compared to sampling methods. 
- This technique combined with a more robust algorithm (random forest, boosting) can lead to exceptionally high accuracy


#### ROSE holdout

```{r}

ROSE.holdout <- ROSE.eval(cls ~ ., data = hacide.train, learner = rpart, method.assess = "holdout", extr.pred = function(obj)obj[,2], seed = 1)

ROSE.holdout


```

- We see that our accuracy retains at ~ 0.98 and shows that our predictions aren't suffering from high variance

#### ROSE LKOCV

```{r}

ROSE.LKOCV <- ROSE.eval(cls ~ ., data = hacide.train, learner = rpart, method.assess = "LKOCV", extr.pred = function(obj)obj[,2], seed = 1,K=5)

ROSE.LKOCV


```

- We see that our accuracy is ~ 0.97


#### ROSE BOOT

```{r}

ROSE.BOOT <- ROSE.eval(cls ~ ., data = hacide.train, learner = rpart, method.assess = "BOOT", extr.pred = function(obj)obj[,2], seed = 1,B=100)

ROSE.BOOT

mean(ROSE.BOOT$acc)

```

- We see that our accuracy is ~ 0.98



#### SMOTE


```{r}

library(DMwR)

data.smote <- SMOTE(cls ~ ., data = hacide.train, perc.over = 2000, perc.under = 105)

table(data.smote$cls)

```


#### build decision tree models and make predictions on unseen data

```{r}
tree.smote <- rpart(cls ~ ., data = data.smote)
pred.tree.smote <- predict(tree.smote, newdata = hacide.test)

```


#### evaluating the model using roc curve

```{r}
#AUC smote
roc.curve(hacide.test$cls, pred.tree.smote[,2])


```



